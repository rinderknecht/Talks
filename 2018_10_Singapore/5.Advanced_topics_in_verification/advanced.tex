%%-*-latex-*-

\documentclass[wide]{slides}

% Language
%
\usepackage{hyphenat}          % \hyp{} is a breakable dash
\usepackage{alltt}             % Verbatim with macros

\frenchspacing               % Follow French conventions after a period

% Maths
%
\usepackage{stmaryrd}
\usepackage{mathpartir}

% Miscellanea
%
\usepackage{url,xspace}

% Input files
%
\input{commands}

% --------------------------------------------------------------------
% Document
%
\maintitle{Advanced Topics in Software Verification}
\mainauthor{\textbf{Christian Rinderknecht}\\
  {\small\url{Christian.Rinderknecht@tezcore.com}}\\
\Nomadic}
\confname{Tezos Masterclass}
\confshortname{Tezos}
\confdate{28 October 2018}

\begin{document}

\maketitle

\begin{slide}
  \title{Introduction}

  \begin{itemize}

    \item This lecture proposes a technical overview of several topics
      in the field of \textbf{software verification}.

    \item It is a mathematical lecture, more specifically,
      \textbf{formal logic} lies at its core.

    \item We will start with perhaps the most obvious guarantee when
      running software: the \textbf{proof of termination}.

    \item Once we know that a program terminates on all its inputs, we
      would like to get an estimation of its running time, depending
      on the inputs: this is the \textbf{analysis of algorithms}.

    \item Finally, we may want to \textbf{optimise a program}. For
      that, we need to \textbf{prove logical properties} about it, and
      perhaps replace it by a better one, so we need \textbf{proofs of
        equivalence} between two programs.

  \end{itemize}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
  \title{Termination}

  \begin{itemize}

    \item When defining our purely functional language, we could
      actually have imposed some syntactic restrictions on recursive
      definitions in order to guarantee the termination of all
      function calls.

    \item A well\hyp{}known class of such terminating functions makes
      exclusively use of a bridled form of recursion called
      \textbf{primitive recursion}.

    \item Unfortunately, many useful functions do not fit easily in
      this framework and, as a consequence, most functional languages
      leave to the programmers the responsibility to check the
      termination of their programs.

    \item \textbf{Termination is an undecidable problem}, that is, it
      is not possible to provide a general criterion for termination,
      but many standard rules exists that cover many usages.

    \item Let us review a few techniques.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Termination}

  \begin{itemize}

    \item Here is again the definition of the factorial function as a
      rewite system:
      \begin{align*}
        \fun{fact}(0) & \rightarrow 1\\
        \fun{fact}(n) & \rightarrow n \times \fun{fact}(n-1)
      \end{align*}

    \item We could use \OCaml to define the factorial, of course, but
      we want to make it clear that what we demonstrate in this
      lecture does not depend on \OCaml itself, but, instead, applies
      to a host of functional languages.

    \item Given a call \(\fun{fact}(n)\), with \(n \geqslant 0\), the
      rewrites yield a series of calls \(\fun{fact}(n-1)\),
      \(\fun{fact}(n-2)\), etc. until \(\fun{fact}(0)\).

    \item Termination here ensues from the \textbf{strictly decreasing
      argument} \(n\), \(n-1\), \(n-2\) etc. down to~\(0\). (No
      infinite descent.)

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Termination / Ackermann's function}

  \begin{itemize}

    \item Consider the following example where \(m,n \in \mathbb{N}\):
      \begin{equation*}
        \begin{array}{@{}r@{\;}l@{\;}l@{}}
          \fun{ack}(0,n)     & \xrightarrow{\theta} & n+1\\
          \fun{ack}(m+1,0)   & \xrightarrow{\smash{\iota}}  &
          \fun{ack}(m,1)\\
          \fun{ack}(m+1,n+1) & \xrightarrow{\smash{\kappa}}
          & \fun{ack}(m,\fun{ack}(m+1,n))
        \end{array}
      \end{equation*}

    \item This is a simplified form of Ackermann's function, an early
      example of a (total) computable function which is \textbf{not
        primitive recursive}. It makes use of double recursion and
      grows values as towers of exponents, for example,
      \[\fun{ack}(4,3) \twoheadrightarrow 2^{2^{65536}} - 3.\]

    \item It is not obviously terminating, because if the first
      argument does decrease or remains the same, the second
      \textbf{largely increases}: we need a stronger criterion to
      prove the termination.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Well\hyp{}founded and lexicographic orders}

  \begin{itemize}

    \item We define a \textbf{well\hyp{}founded order} on a set~\(A\)
      as a binary relation~\((\succ)\) which does not have any
      \textbf{infinite descending chains}, to wit, there is no \(a_0
      \succ a_1 \succ \dots\)

    \item Let us define a well\hyp{}founded order on \textbf{pairs},
      called \textbf{lexicographic order}. Let~\((\succ_A)\) and
      \((\succ_B)\) be well\hyp{}founded orders on the sets \(A\)
      and~\(B\).

    \item Then \((\succ_{A \times B})\) defined as follows on \(A
      \times B\) is well\hyp{}founded:
      \begin{equation*}
        (a_0,b_0) \succ_{A \times B} (a_1,b_1) :\Leftrightarrow
        \text{\(a_0 \succ_A a_1\) or (\(a_0 = a_1\) and \(b_0 \succ_B
          b_1\)).}
      \end{equation*}

    \item For example, \((\underline{1},7) \prec (\underline{2},3)\),
      \((1,\underline{7}) \succ (1,\underline{3})\).

    \item If \(A=B=\mathbb{N}\) then \((\succ_A) = (\succ_B) =
      (>)\).

    \item To prove that \(\fun{ack}(m,n)\) terminates for all \(m,n
      \in \mathbb{N}\), first, we must find a well\hyp{}founded order
      on the calls \(\fun{ack}(m,n)\).

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Termination / Ackermann's function}

  \begin{itemize}

    \item That is to say, the calls must be totally ordered without
      any infinite descending chain. In this case, a lexicographic
      order on \((m,n) \in \mathbb{N}^2\) extended to
      \(\fun{ack}(m,n)\) works:
      \begin{equation*}
        \fun{ack}(a_0,b_0) \; \succ \; \fun{ack}(a_1,b_1) \;\;
        :\Leftrightarrow \;\;
        \text{\(a_0 > a_1\) or (\(a_0 = a_1\) and \(b_0 > b_1\)).}
      \end{equation*}

    \item Clearly, \(\fun{ack}(0,0)\) is the minimum element.

    \item Second, we must prove that ``\fun{ack}'' rewrites to smaller
      calls.

    \item We are only concerned with rules \(\iota\)~and~\(\kappa\).

    \item With the former, we have \(\fun{ack}(m+1,0) \succ
      \fun{ack}(m,1)\).

    \item With the latter, we have
      \begin{equation*}
        \begin{array}{@{}r@{\;}l@{\;}l@{}}
          \fun{ack}(m+1,n+1) & \succ & \fun{ack}(m+1,n)\\
          \fun{ack}(m+1,n+1) & \succ & \fun{ack}(m,p),
        \end{array}
      \end{equation*}
      for all values~\(p\), in particular when \(\fun{ack}(m+1,n)
      \twoheadrightarrow p\).\hfill\(\Box\)

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Termination / Dependency pairs and subterm order}

  \begin{itemize}

    \item In general, the approach often taken to prove termination
      consists in finding a well\hyp{}founded order \((\succ)\) on the
      rewritten terms, which is entailed by the rewrite relation
      \((\rightarrow)\), that is,
      \begin{equation*}
        x \; \rightarrow \; y \;\; \Rightarrow \;\; x \; \succ \; y.
      \end{equation*}

    \item As we have seen, we do not always need to order the terms
      \(x\)~and~\(y\): only the calls in them to a given
      function. These are \textbf{dependency pairs}. Calls to other
      functions can be assumed to be terminating.

    \item With the factorial and Ackermann's function, we have seen
      how to deal with one or more integer arguments to these calls.

    \item What about other data types, like lists and trees?

    \item One well\hyp{}founded order for lists (stacks) is the
      \textbf{immediate subterm order}, satisfying
      \begin{equation*}
        \cons{x}{s} \; \succ \; s \quad
        \text{and} \quad \cons{x}{s} \; \succ \; x.
      \end{equation*}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Termination / Proper subterm order}

  \begin{itemize}

    \item The immediate subterm order simply means that a list is
      ``greater'' than its first element and the rest of the list
      (called the \textbf{tail}).

    \item It is a special case of the \textbf{proper subterm order},
      in which a structured value is ``greater'' than any of its
      components, wherever they are located.

    \item These orders are convenient to prove the termination of
      calls to functions whose definitions are recursive in terms of
      substructures of their arguments: this is quite a common case.

    \item For example, let us recall the function catenating a list to
      another:
      \begin{align*}
        \fun{cat}(        \el,t) &\xrightarrow{\alpha} t\\
        \fun{cat}(\cons{x}{s},t) &\xrightarrow{\smash{\beta}}
        \cons{x}{\fun{cat}(s,t)}
      \end{align*}
      We simply have
      \begin{equation*}
        \cons{x}{s} \; \succ \; s \;\; \Rightarrow \;\;
        \fun{cat}(\cons{x}{s},t) \; \succ \;
        \fun{cat}(s,t).
      \end{equation*}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Termination / List flattening}

  \begin{itemize}

    \item Let us try a more complicated example:
      \begin{equation*}
        \begin{array}{r@{\;}l@{\;}l}
          \fun{flat}(\el)                   & \xrightarrow{\psi}
          & \el\\
          \fun{flat}(\cons{\el}{t})         & \xrightarrow{\smash{\omega}}
          & \fun{flat}(t)\\
          \fun{flat}(\cons{(\cons{x}{s})}{t}) & \xrightarrow{\smash{\gamma}}
          & \fun{cat}(\fun{flat}(\cons{x}{s}),\fun{flat}(t))\\
          \fun{flat}(\cons{y}{t})           & \xrightarrow{\smash{\delta}}
          & \cons{y}{\fun{flat}(t)}
        \end{array}
      \end{equation*}

    \item Some evaluations:
      \begin{align*}
        \fun{flat}(\el) & \twoheadrightarrow \el\\
        \fun{flat}([\el;[\el]]) & \twoheadrightarrow \el\\
        \fun{flat}([\el;[1;[2;\el];3];\el]) & \twoheadrightarrow [1;2;3]
      \end{align*}

    \item What ''\fun{flat}'' does is flatten a list of lists
      arbitrarily deep.

    \item Note that this function cannot be written in \OCaml because
      the lists in question are not homogeneous.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Termination / List flattening}

  A detailed evaluation:
    \begin{equation*}
      \begin{array}{r@{\;}l@{\;}l}
        \fun{flat}([\el;[[1];2];3])
        & \xrightarrow{\omega}
        & \fun{flat}([[[1];2];3])\\
        & \xrightarrow{\smash{\gamma}}
        & \fun{cat}(\underline{\fun{flat}([[1];2])},\fun{flat}([3]))\\
        & \xrightarrow{\smash{\gamma}}
        & \fun{cat}(\fun{cat}(\underline{\fun{flat}([1])},\fun{flat}([2])),
        \fun{flat}([3]))\\
        & \xrightarrow{\smash{\delta}}
        & \fun{cat}(\fun{cat}(\cons{1}{\underline{\fun{flat}(\el)}},
        \fun{flat}([2])), \fun{flat}([3]))\\
        & \xrightarrow{\smash{\psi}}
        & \fun{cat}(\fun{cat}([1],\underline{\fun{flat}([2])}),
        \fun{flat}([3]))\\
        & \xrightarrow{\smash{\delta}}
        & \fun{cat}(\fun{cat}([1],\cons{2}{\underline{\fun{flat}(\el)}}),
        \fun{flat}([3]))\\
        & \xrightarrow{\smash{\psi}}
        & \fun{cat}(\fun{cat}([1],[2]),\underline{\fun{flat}([3])})\\
        & \xrightarrow{\smash{\delta}}
        & \fun{cat}(\fun{cat}([1],[2]),\cons{3}{\underline{\fun{flat}(\el)}})\\
        & \xrightarrow{\smash{\psi}}
        & \fun{cat}(\fun{cat}([1],[2]),[3])\\
        & \twoheadrightarrow & [1;2;3].
      \end{array}
    \end{equation*}

\end{slide}


\begin{slide}
  \title{Termination / List flattening}

  \begin{itemize}

    \item The function ``\fun{cat}'' is independent of ``\fun{flat}''
      and we proved its termination in isolation by using the proper
      subterm order on its first argument.

    \item Assuming now that ``\fun{cat}'' terminates, let us prove the
      termination of ``\fun{flat}''.

    \item Because the recursive calls to ``\fun{flat}'' contain only
      (list) constructors, we can try to order their arguments.

    \item The proper subterm order works:

      \begin{tabular}{r@{\;\,}c@{\;}ll}
        \(\cons{y}{t}\) & \(\succ\) & \(t\) & (rule~\(\delta\)
          and rule~\(\omega\) when \(y=\el\))\\
        \(\cons{(\cons{x}{s})}{t}\) & \(\succ\) & \(t\)
          & (rule~\(\gamma\))\\
        \(\cons{(\cons{x}{s})}{t}\) & \(\succ\) & \(\cons{x}{s}\)
          & (rule~\(\gamma\))
      \end{tabular}

    \item Termination ensues.\hfill\(\Box\)

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Termination / List flattening}

  \begin{itemize}

    \item Let us consider now an alternative design for
      rule~\(\gamma\):
      \begin{equation*}
          \begin{array}{@{}r@{\;}c@{\;}l@{}}
            \fun{flat}(\el) & \xrightarrow{\psi}
                                      & \el\\
            \fun{flat}(\cons{\el}{t}) & \xrightarrow{\smash{\omega}}
                                      & \fun{flat}(t)\\
            \fun{flat}(\cons{(\cons{x}{s})}{t})
                                      & \xrightarrow{\smash{\gamma}}
                                      & \fbox{\(\fun{flat}(\cons{x}{\cons{s}{t}})\)}\\
            \fun{flat}(\cons{y}{t})   & \xrightarrow{\smash{\delta}}
                                      & \cons{y}{\fun{flat}(t)}
          \end{array}
        \end{equation*}

    \item This amounts to perform a \textbf{right rotation} of the
      abstract syntax tree of the term:
      \begin{center}
        \includegraphics[trim=4mm 6cm 8cm 0]{flat_rotr.pdf}
      \end{center}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Termination / Measures}

  \begin{itemize}

    \item Here, the proper subterm order fails:
      \begin{equation*}
        \cons{(\cons{x}{s})}{t} \;\; \nsucc \;\;
        \cons{x}{(\cons{s}{t})} = \cons{x}{\cons{s}{t}}
      \end{equation*}
      because \(t \;\; \nsucc \;\; \cons{s}{t}\).

    \item We need another method to prove termination:
      \textbf{measures}.

    \item A measure \(\measure{\cdot}\) is a map from terms to a
      well\hyp{}ordered set (\(A,\succ\)), which is
      \textbf{monotonically increasing} with respect to the rewrite
      relation:
      \begin{equation*}
        x \; \rightarrow \; y \;\; \Rightarrow \;\; \measure{x} \;\,
        \succ \; \measure{y} \;\; \Rightarrow \;\; x\; \succ\; y.
      \end{equation*}

    \item Actually, as earlier, we will only consider
      \textbf{dependency pairs}, that is, pairs of calls whose first
      component is the left\hyp{}hand side of a rule and the second
      components are the calls in the righ\hyp{}hand side of the same
      rule.

  \end{itemize}


\end{slide}

\begin{slide}
  \title{Termination / Polynomial measures}

  \begin{itemize}

    \item The pairs are
      \begin{enumerate}

        \item \((\fun{flat}(\cons{\el}{t}), \fun{flat}(t))_\omega\)

        \item \((\fun{flat}(\cons{(\cons{x}{s})}{t}),
          \fun{flat}(\cons{x}{\cons{s}{t}}))_\gamma\)

        \item \((\fun{flat}(\cons{y}{t}), \fun{flat}(t))_\delta\)

      \end{enumerate}
      with \(y \not\in S\), that is, \(y\)~is not a list.

    \item We can actually drop the function names, as all the pairs
      involve ``\fun{flat}''.

    \item A common class of measures are monotonically increasing
      embeddings into (\(\mathbb{N},>\)), so let us seek a measure
      satisfying:
      \begin{center}
        \begin{tabular}{r@{\;\,}c@{\;}ll}
          \(\measure{\cons{(\cons{x}{s})}{t}}\) & > &
          \(\measure{\cons{x}{\cons{s}{t}}}\)\\
          \(\measure{\cons{y}{t}}\) & > & \(\measure{t}\),
          & if \(y \not\in S\) or \(y=\el\).
        \end{tabular}
      \end{center}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Termination / Polynomial measures}

  \begin{itemize}

    \item For instance, let us set the following \textbf{polynomial
      measure}:
      \begin{center}
        \begin{tabular}{r@{\;\,}c@{\;\;}ll}
          \(\measure{\cons{x}{s}}\) & := & \(1 + 2\cdot\measure{x} +
          \measure{s}\)\\
          \(\measure{y}\) & := & \(0\), & if \(y \not\in S\) or
          \(y=\el\).
        \end{tabular}
      \end{center}

    \item We have
      \begin{align*}
        \measure{\cons{(\cons{x}{s})}{t}} & \;\; = \;\; 3 +
        4\cdot\measure{x} + 2\cdot\measure{s} + \measure{t}\\
        \measure{\cons{x}{\cons{s}{t}}} & \;\; = \;\; 2 +
        2\cdot\measure{x} + 2\cdot\measure{s} + \measure{t}
      \end{align*}

    \item Then
      \begin{center}
        \begin{tabular}{r@{\;\;}c@{\;\;}l}
          \(\measure{\cons{(\cons{x}{s})}{t}}\) & >
          & \(\measure{\cons{x}{\cons{s}{t}}}\)\\
          \(\measure{\cons{y}{t}} = 1 + \measure{t}\) & > &
          \(\measure{t}\)
        \end{tabular}
      \end{center}

    \item Since \(\measure{x} \in \mathbb{N}\), for all~\(x\), these
      inequalities entail the termination of ``\fun{flat}'' (no
      infinite descent).\hfill\(\Box\)

  \end{itemize}

\end{slide}


%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
  \title{Analysis of algorithms}

  \begin{itemize}

    \item The mathematical study of the efficiency of algorithms has
      been pioneered by Knuth, who called it \textbf{analysis of
        algorithms}.

    \item Given a function definition,
      \begin{enumerate}

        \item we define a measure on the arguments, which represents
          their size;

        \item we define a measure on time, which abstracts the wall
          clock time;

        \item we express the abstract time needed to compute calls to
          that function in terms of the size of its arguments: this is
          the \textbf{cost function}.

      \end{enumerate}

    \item The lower the cost, the higher the efficiency.

    \item For example, for sorting objects, called \textbf{keys}, by
      comparing them,
      \begin{enumerate}

        \item the input size is the number of keys,

        \item the abstract unit of time is one comparison,

        \item and the cost maps the number of keys to the number of
          comparisons to sort the keys.

      \end{enumerate}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Exact cost}

  \begin{itemize}

    \item Rewrite systems enable a rather natural notion of cost for
      functional programs: it is the number of rewrites to reach the
      value of a function call, assuming that the arguments are
      values.

    \item In other words, the \textbf{exact cost} is the number of
      calls needed to compute the value of a given call.

    \item For stacks, the measure of the size of the input is the
      number of items it contains, or \textbf{length} of the stack.

  \end{itemize}

\end{slide}


\begin{slide}
  \title{Exact cost / Appending a stack}

  \begin{itemize}

    \item For instance, let us recall the appending of a stack:
      \begin{align*}
        \fun{cat}(        \el,t) &\xrightarrow{\alpha} t\\
        \fun{cat}(\cons{x}{s},t) &\xrightarrow{\smash{\beta}}
        \cons{x}{\fun{cat}(s,t)}
      \end{align*}

    \item We observe that~\(t\) is invariant, so the cost depends only
      on the size of the first argument.

    \item Let \(\C{\fun{cat}}{n}\) be the cost of the call
      \(\fun{cat}(s,t)\), for any~\(t\), where \(n\)~is the size
      of~\(s\).

    \item Each rule yields an equation on their cost:
      \begin{align*}
        \C{\fun{cat}}{0} & \eqn{\alpha} 1\\
        \C{\fun{cat}}{n+1} & \eqn{\smash{\beta}} 1
        + \C{\fun{cat}}{n}.
      \end{align*}

      \item Together, they yield \(\C{\fun{cat}}{n} = n + 1.\) Why?

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Why?}

  The trick consists in making the difference of two successive terms
  in the series and then summing up these differences:
\begin{align*}
\C{\fun{cat}}{n+1} - \C{\fun{cat}}{n} &= 1 & \text{where} \;\, n
\geqslant 0\\
\sum_{n=0}^{p}{(\C{\fun{cat}}{n+1} - \C{\fun{cat}}{n})} &=
\sum_{n=0}^{p}{1} & \text{where} \;\, p \geqslant 0\\
\C{\fun{cat}}{p+1} - \C{\fun{cat}}{0} &= p + 1\\
\intertext{Replacing \(p+1\) by \(n\) we finally get}
\C{\fun{cat}}{n} - \C{\fun{cat}}{0} &= n & \text{where}\;\, n > 0\\
\C{\fun{cat}}{n}  &= n + 1
\end{align*}
Since that replacing \(n\) by \(0\) in the latter formula gives
\(\C{\fun{cat}}{0} = 0 + 1 = 1\), we can have only one formula for the
cost: \(\C{\fun{cat}}{n} = n + 1,\) where \(n \geqslant 0.\)\hfill\(\Box\)

\end{slide}

\begin{slide}
  \title{Exact cost / Reversal of a list}

  \begin{itemize}

    \item Let us consider the definition of a function \fun{rev\(_0\)}
      reversing a stack:
      \begin{equation*}
        \begin{array}{@{}r@{\;}l@{\;}lr@{\;}l@{\;}l@{}}
          \fun{cat}(\el,t)
          & \xrightarrow{\alpha} & t
          & \fun{rev}_0(\el)
          & \xrightarrow{\smash{\gamma}} & \el\\
          \fun{cat}(\cons{x}{s},t)
          & \xrightarrow{\smash{\beta}} & \cons{x}{\fun{cat}(s,t)}
          & \fun{rev}_0(\cons{x}{s})
          & \xrightarrow{\smash{\delta}} & \fun{cat}(\fun{rev}_0(s),[x])
        \end{array}
      \end{equation*}

      \item The evaluation of \(\fun{rev}_0([3;2;1])\) is shown with
        abstract syntax trees:
        \begin{center}
          \includegraphics[trim=4mm 5cm 2cm 0]{rev0_321_0.pdf}
        \end{center}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Exact cost / Reversal of a list}

  \begin{itemize}

    \item Here are the definitions again:
      \begin{equation*}
        \begin{array}{@{}r@{\;}l@{\;}lr@{\;}l@{\;}l@{}}
          \fun{cat}(\el,t)
          & \xrightarrow{\alpha} & t
          & \fun{rev}_0(\el)
          & \xrightarrow{\smash{\gamma}} & \el\\
          \fun{cat}(\cons{x}{s},t)
          & \xrightarrow{\smash{\beta}} & \cons{x}{\fun{cat}(s,t)}
          & \fun{rev}_0(\cons{x}{s})
          & \xrightarrow{\smash{\delta}} & \fun{cat}(\fun{rev}_0(s),[x])
        \end{array}
      \end{equation*}

      \item Let us resume the evaluation of \(\fun{rev}_0([3;2;1])\)
        from where we left:
        \begin{center}
          \includegraphics[trim=4mm 5cm 2cm 0]{rev0_321_1.pdf}
        \end{center}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Exact cost / Reversal of a list}

  \begin{itemize}

    \item Here are the definitions again:
      \begin{equation*}
        \begin{array}{@{}r@{\;}l@{\;}lr@{\;}l@{\;}l@{}}
          \fun{cat}(\el,t)
          & \xrightarrow{\alpha} & t
          & \fun{rev}_0(\el)
          & \xrightarrow{\smash{\gamma}} & \el\\
          \fun{cat}(\cons{x}{s},t)
          & \xrightarrow{\smash{\beta}} & \cons{x}{\fun{cat}(s,t)}
          & \fun{rev}_0(\cons{x}{s})
          & \xrightarrow{\smash{\delta}} & \fun{cat}(\fun{rev}_0(s),[x])
        \end{array}
      \end{equation*}

      \item Let us resume the evaluation of \(\fun{rev}_0([3;2;1])\)
        from where we left:
        \begin{center}
          \includegraphics[trim=4mm 5cm 2cm 0]{rev0_321_1_1.pdf}
        \end{center}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Exact cost / Reversal of a list}

  \begin{itemize}

    \item Here are the definitions again:
      \begin{equation*}
        \begin{array}{@{}r@{\;}l@{\;}lr@{\;}l@{\;}l@{}}
          \fun{cat}(\el,t)
          & \xrightarrow{\alpha} & t
          & \fun{rev}_0(\el)
          & \xrightarrow{\smash{\gamma}} & \el\\
          \fun{cat}(\cons{x}{s},t)
          & \xrightarrow{\smash{\beta}} & \cons{x}{\fun{cat}(s,t)}
          & \fun{rev}_0(\cons{x}{s})
          & \xrightarrow{\smash{\delta}} & \fun{cat}(\fun{rev}_0(s),[x])
        \end{array}
      \end{equation*}

      \item Let us resume the evaluation of \(\fun{rev}_0([3;2;1])\)
        from where we left:
        \begin{center}
          \includegraphics[trim=4mm 5cm 2cm 0]{rev0_321_2.pdf}
        \end{center}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Exact cost / Reversal of a list}

  \begin{itemize}

    \item Here are the definitions again:
      \begin{equation*}
        \begin{array}{@{}r@{\;}l@{\;}lr@{\;}l@{\;}l@{}}
          \fun{cat}(\el,t)
          & \xrightarrow{\alpha} & t
          & \fun{rev}_0(\el)
          & \xrightarrow{\smash{\gamma}} & \el\\
          \fun{cat}(\cons{x}{s},t)
          & \xrightarrow{\smash{\beta}} & \cons{x}{\fun{cat}(s,t)}
          & \fun{rev}_0(\cons{x}{s})
          & \xrightarrow{\smash{\delta}} & \fun{cat}(\fun{rev}_0(s),[x])
        \end{array}
      \end{equation*}

      \item Let us resume the evaluation of \(\fun{rev}_0([3;2;1])\)
        from where we left:
        \begin{center}
          \includegraphics[trim=4mm 5cm 2cm 0]{rev0_321_2_1.pdf}
        \end{center}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Exact cost / Reversal of a list}

  \begin{itemize}

    \item Here are the definitions again:
      \begin{equation*}
        \begin{array}{@{}r@{\;}l@{\;}lr@{\;}l@{\;}l@{}}
          \fun{cat}(\el,t)
          & \xrightarrow{\alpha} & t
          & \fun{rev}_0(\el)
          & \xrightarrow{\smash{\gamma}} & \el\\
          \fun{cat}(\cons{x}{s},t)
          & \xrightarrow{\smash{\beta}} & \cons{x}{\fun{cat}(s,t)}
          & \fun{rev}_0(\cons{x}{s})
          & \xrightarrow{\smash{\delta}} & \fun{cat}(\fun{rev}_0(s),[x])
        \end{array}
      \end{equation*}

      \item Let us resume the evaluation of \(\fun{rev}_0([3;2;1])\)
        from where we left:
        \begin{center}
          \includegraphics[trim=4mm 5cm 2cm 0]{rev0_321_3.pdf}
        \end{center}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Exact cost / Reversal of a list}

  \begin{itemize}

    \item Here are the definitions again:
      \begin{equation*}
        \begin{array}{@{}r@{\;}l@{\;}lr@{\;}l@{\;}l@{}}
          \fun{cat}(\el,t)
          & \xrightarrow{\alpha} & t
          & \fun{rev}_0(\el)
          & \xrightarrow{\smash{\gamma}} & \el\\
          \fun{cat}(\cons{x}{s},t)
          & \xrightarrow{\smash{\beta}} & \cons{x}{\fun{cat}(s,t)}
          & \fun{rev}_0(\cons{x}{s})
          & \xrightarrow{\smash{\delta}} & \fun{cat}(\fun{rev}_0(s),[x])
        \end{array}
      \end{equation*}

    \item Let \(\C{\fun{rev}_0}{k}\) be the cost of evaluating a call
      \(\fun{rev}_0(l)\), where the list~\(l\) has length~\(k\).

    \item Let \(\C{\fun{cat}}{k}\) be the cost of evaluating a call
      \(\fun{cat}(l)\), where the list~\(l\) has length~\(k\).

    \item The definition of \fun{rev\(_0\)} directly leads to the
      recurrence equations
      \begin{align*}
        \C{\fun{rev}_0}{0}   & \eqn{\gamma} 1\\
        \C{\fun{rev}_0}{k+1} & \eqn{\smash{\delta}} 1 +
        \C{\fun{rev}_0}{k} + \C{\fun{cat}}{k} = \C{\fun{rev}_0}{k} + k
        + 2
      \end{align*}
      because the length of \(\fun{rev}_0(s)\) is~\(k\) if the length
      of~\(s\) is~\(k\), and we already know \(\C{\fun{cat}}{k} = k +
      1\).

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Exact cost / Reversal of a list}

  \begin{itemize}

    \item We have
      \begin{equation*}
        \sum_{k=0}^{n-1}(\C{\fun{rev}_0}{k+1} - \C{\fun{rev}_0}{k})
        \;\; = \;\;
        \C{\fun{rev}_0}{n} - \C{\fun{rev}_0}{0} \;\; = \;\;
        \sum_{k=0}^{n-1}(k+2) \;\; = \;\; 2n + \sum_{k=0}^{n-1}{k}.
      \end{equation*}

    \item The remaining sum is a classic of algebra:
      \begin{equation*}
        2 \cdot \sum_{k=0}^{n-1}{k} \;\; = \;\;
        \sum_{k=0}^{n-1}{k} + \sum_{k=0}^{n-1}{k} \;\; = \;\;
        \sum_{k=0}^{n-1}{k} + \sum_{k=0}^{n-1}(n-k-1) \;\; = \;\; n(n-1).
      \end{equation*}

    \item Consequently,
      \begin{equation*}
        \sum_{k=0}^{n-1}{k} \;\; = \;\; \frac{1}{2} n(n-1).
      \end{equation*}

    \item We can finally conclude
      \begin{equation*}
        \C{\fun{rev}_0}{n} \;\; = \;\; \C{\fun{rev}_0}{0} + 2n +
        \frac{1}{2} n(n-1)
        \;\; = \;\; \frac{1}{2}n^2 + \frac{3}{2}n + 1
        \;\; \sim \;\; \frac{1}{2}n^2.
      \end{equation*}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Exact cost / Evaluation traces}

  \begin{itemize}

    \item Another way to reach the result is to induce an
      \textbf{evaluation trace}. A trace is a composition of rewrite
      rules, noted using the mathematical convention for
      multiplication.

    \item From the evaluation of \(\fun{rev}_0([3;3;1])\), we infer
      the trace \(\T{\fun{rev}_0}{n}\) of the evaluation of
      \(\fun{rev}_0(s)\), where \(n\)~is the length of~\(s\):
      \begin{equation*}
        \T{\fun{rev}_0}{n} \;\; \text{:=} \;\;
        \delta^n\gamma\alpha(\beta\alpha)\dots(\beta^{n-1}\alpha) \;\;
        = \;\; \delta^n\gamma \prod_{k=0}^{n-1}{\beta^k\alpha}.
      \end{equation*}

    \item This reads: ``Apply \(\delta\) \(n\)~times, then \(\gamma\),
      then \(\alpha\) etc.''

    \item If we note \(\len{\T{\fun{rev}_0}{n}}\) the length
      of~\(\T{\fun{rev}_0}{n}\), that is, the number of rule
      applications it contains, we expect to have the equations
      \(\len{x} = 1\), for a rule~\(x\), and \(\len{x \cdot y} =
      \len{x} + \len{y}\), for rules \(x\)~and~\(y\).

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Exact cost / Evaluation traces}

  \begin{itemize}

    \item By definition of the cost:
      \begin{align*}
        \C{\fun{rev}_0}{n}
        & \;\; \text{:=} \;\; \len{\T{\fun{rev}_0}{n}}
        \;\; = \;\; \left\lvert\delta^n\gamma
        \prod_{k=0}^{n-1}{\beta^k\alpha}\right\lvert
        \;\; = \;\;
        \len{\delta^n\gamma} + \sum_{k=0}^{n-1}\len{\beta^k\alpha}\\
        & \;\; = \;\; (n+1) + \sum_{k=0}^{n-1}(k+1)
        \;\; = \;\; (n+1) + \sum_{k=1}^{n}k
        \;\; = \;\; \frac{1}{2}n^2 + \frac{3}{2}n + 1
        \;\; \sim \;\; \frac{1}{2}n^2.
      \end{align*}

    \item Since we inferred our trace from an example, the cost
      function we derived may be wrong: we need to prove it by
      \textbf{induction}.

    \item Let \(\pred{Quad}{n}\) be the property \(\C{\fun{rev}_0}{n}
      \;\; = \;\; (n^2 + 3n + 2)/2\). We already know that
      \(\pred{Quad}{0}\) is true. Let us suppose \(\pred{Quad}{n}\) is
      true for some value of~\(n\) (induction hypothesis) and let us
      prove \(\pred{Quad}{n+1}\).

    \item We know \(\C{\fun{rev}_0}{n+1} \;\; = \;\;
      \C{\fun{rev}_0}{n} + n + 2\). The induction hypothesis implies
      \begin{equation*}
        \C{\fun{rev}_0}{n+1} \;\; = \;\; (n^2 + 3n + 2)/2 + n + 2 \;\;
        = \;\; ((n+1)^2 + 3(n+1) + 2)/2,
      \end{equation*}
      which is \(\pred{Quad}{n+1}\). Therefore, the induction
      principle says that the cost we found is always
      correct.\hfill\(\Box\)

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Exact cost / Empirical approach}

  \begin{itemize}

    \item Yet another way to determine the cost of \fun{rev\(_0\)}
      consists in first \textbf{guessing} that it is quadratic, that
      is, \(\C{\fun{rev}_0}{n} \;\; = \;\; an^2 + bn + c\), where
      \(a\), \(b\) and~\(c\) are unknowns.

    \item Since there are three coefficients, we only need three
      values of~\(\C{\fun{rev}_0}{n}\) to determine them, for instance
      \(n = 0, 1, 2\).

    \item Making some traces, we find
      \begin{equation*}
        \C{\fun{rev}_0}{0} = 1, \;\; \C{\fun{rev}_0}{1} = 3 \;\;\;
        \text{and} \;\;\; \C{\fun{rev}_0}{2} = 6
      \end{equation*}
      so we solve the linear system
      \begin{equation*}
        \C{\fun{rev}_0}{0} = c \; = \; 1,\quad
        \C{\fun{rev}_0}{1} = a + b + c \; = \; 3,\quad
        \C{\fun{rev}_0}{2} = a \cdot 2^2 + b \cdot 2 + c \; = \; 6.
      \end{equation*}

    \item Hence \(a = \myfrac1/2\), \(b = \myfrac3/2\) and~\(c = 1\),
      that is, \(\C{\fun{rev}_0}{n} = (n^2 + 3n + 2)/2\).

    \item We may have been wrong with our guess of a quadratic cost:
      we need also here to prove it by \textbf{induction} exactly as
      before.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Exact cost / Efficient reversal}

  \begin{itemize}

    \item We would be remiss not to consider a more efficient (that
      is, less costly) definition of a reversal function.

    \item Here it is:
      \begin{align*}
        \fun{rev}(s) &\xrightarrow{\epsilon} \fun{rcat}(s,\el)\\
        \fun{rcat}(\el,t) & \xrightarrow{\smash{\zeta}} t\\
        \fun{rcat}(\cons{x}{s},t) &\xrightarrow{\smash{\eta}}
        \fun{rcat}(s,\cons{x}{t})
      \end{align*}
      (The name ``rcat'' stands for \emph{reverse and catenate}.)

    \item The additional parameter introduced by ``\fun{rcat}''
      accumulates partial results, thus is called an
      \textbf{accumulator}:
      \begin{equation*}
        \begin{array}{r@{\;}l@{\;}l}
          \fun{rev}([3;2;1])
          & \xrightarrow{\epsilon} & \fun{rcat}([3;2;1],\el)\\
          & \xrightarrow{\smash{\eta}}     & \fun{rcat}([2;1],[3])\\
          & \xrightarrow{\smash{\eta}}     & \fun{rcat}([1],[2;3])\\
          & \xrightarrow{\smash{\eta}}     & \fun{rcat}(\el,[1;2;3])\\
          & \xrightarrow{\smash{\zeta}}    & [1;2;3].
        \end{array}
      \end{equation*}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Exact cost / Efficient reversal}

  \begin{itemize}

    \item To draw the cost of \fun{rev}, it is sufficient to notice
      that the first argument of \fun{rcat} strictly decreases at each
      rewrite, so all evaluation traces have the shape
      \(\epsilon\eta^n\zeta\), so \(\C{\fun{rev}}{n} = n + 2\).

    \item The cost is \textbf{linear}, so \fun{rev} must always be
      used instead of \fun{rev\(_0\)}.

    \item The reason for the inefficiency of \fun{rev\(_0\)} can be
      seen in the fact that rule~\clause{\delta} produces a series of
      calls to ``\fun{cat}'' following the
      pattern
      \begin{equation*}
        \fun{rev}_0(s) \twoheadrightarrow \fun{cat}(\fun{cat}(\dots
        \fun{cat}(\el, [x_n]), \dots, [x_2]), [x_1]),
      \end{equation*}
      where \(s = [x_1, x_2, \dots, x_n]\).

    \item The cost of all these calls to ``\fun{cat}'' is thus
      \begin{equation*}
        1 + 2 + \dots + (n-1) = \tfrac{1}{2}n(n-1).
      \end{equation*}
      because the cost of \(\fun{cat}(s,t)\) is \(n+1\).

    \item The problem is not calling ``\fun{cat}'', but the fact that
      the calls are embedded in the most unfavourable configuration.
      (Cats are not evil.)

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Extremal costs}

  \begin{itemize}

    \item When considering sorting programs based on comparisons, the
      cost varies depending on the algorithm and it also often depends
      on the original partial ordering of the keys.

    \item Therefore, size does not capture all aspects needed to
      assess efficiency.

    \item This quite naturally leads to consider bounds on the cost:
      for a given input size, we seek the configurations of the input
      that minimise and maximise the cost, respectively called the
      \begin{itemize}

        \item \textbf{minimum cost} (cost for the \textbf{best case})
          and

        \item \textbf{maximum cost} (cost for the \textbf{worst
          case}).

      \end{itemize}

    \item For example, some sorting algorithms have their worst case
      when the keys are already sorted, others when they are sorted in
      reverse order, etc.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Average cost}

  \begin{itemize}

    \item Once we obtain bounds on a cost, the question about the
      \textbf{average} or \textbf{mean cost} (also called
      \textbf{expected cost}) arises as well.

    \item It is the arithmetic mean of the costs for all possible
      inputs of a given size.

    \item Some care is necessary, as there must be a finite number of
      such inputs.

    \item For instance, to assess the mean cost of sorting algorithms
      based on comparisons, it is usual to assume
      \begin{enumerate}

        \item that the input is a series of \(n\)~\textbf{distinct
          keys} and

        \item that the sum of the costs is taken over all their
          \textbf{permutations}, thus divided by~\(n!\), the number of
          permutations of size~\(n\).

      \end{enumerate}

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Average cost}

  \begin{itemize}

    \item The uniqueness constraint actually allows the analysis to
      equivalently, and more simply, consider the permutations of
      \((1,2,\dots,n)\).

    \item Some sorting algorithms, like \textbf{merge sort} or
      \textbf{insertion sort}, have their average cost
      \textbf{asymptotically equivalent} to their maximum cost.

    \item In other words, for increasingly large numbers of keys, the
      ratio of the two costs become arbitrarily close to~\(1\).

    \item Some others, like \textbf{Hoare's sort}, also known as
      \textbf{quicksort}, have the growth rate of their average cost
      being of a lower magnitude than the maximum cost, on an
      asymptotic scale.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Amortised cost}

  \begin{itemize}

    \item Sometimes an update is costly because it is delayed by an
      imbalance in the data structure that calls for an immediate
      remediation, but this remediation itself may lead to a state
      such that subsequent operations are faster than if the costly
      update had not happen.

    \item Therefore, when considering a series of updates, it may be
      overly pessimistic to sum the maximum costs of all the
      operations considered in isolation.

    \item Instead, \textbf{amortised analysis} takes into account the
      interactions between updates, so a lower maximum bound on the
      cost is derived.

    \item Note that this kind of analysis is inherently different from
      the average case analysis, as its object is the composition of
      different functions instead of independent calls to the same
      function on different inputs. Amortised analysis is a
      worst case analysis of a sequence of updates.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Equivalence classes of terms w.r.t. evaluation}

  \begin{itemize}

    \item We may remark that
      \begin{equation*}
        \fun{cat}([1], [2;3;4]) \;\; \twoheadrightarrow \;\; [1;2;3;4]
        \;\; \twoheadleftarrow \;\; \fun{cat}([1;2], [3;4])
      \end{equation*}

    \item It is enlightening to create \textbf{equivalence classes} of
      terms that are joinable, based on the \textbf{equivalence
        relation}~\((\equiv)\) defined as: \(a \;\; \equiv \;\; b\) if
      there exists a value~\(v\) such that \(a \twoheadrightarrow v\)
      and \(b \twoheadrightarrow v\).

    \item For instance, above: \(\fun{cat}([1], [2;3;4]) \;\; \equiv
      \;\; \fun{cat}([1;2], [3;4])\).

    \item The relation~(\(\equiv\)) is indeed an equivalence because
      it is
      \begin{itemize}

        \item \textbf{reflexive}: \(a \;\; \equiv \;\; a\);

        \item \textbf{symmetric}: if \(a \;\; \equiv \;\; b\), then
          \(b \;\; \equiv \;\; a\);

        \item \textbf{transitive}: if \(a \;\; \equiv \;\; b\) and \(b
          \;\; \equiv \;\; c\), then \(a \;\; \equiv \;\; c\).

      \end{itemize}

      \item If we want to prove equivalences with variables ranging
        over infinite sets, like
        \begin{equation*}
          \forall s,t,u \in \mathcal{S}. \fun{cat}(s,\fun{cat}(t,u))
          \;\; \equiv \;\; \fun{cat}(\fun{cat}(s,t),u)
        \end{equation*}
        we need some \textbf{induction principle}.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Well\hyp{}founded induction}

  \begin{itemize}

    \item We define a \textbf{well\hyp{}founded order} on a set~\(A\)
      as being a binary relation~\((\succ)\) which does not have any
      \textbf{infinite descending chains}, to wit, there is no \(a_0
      \;\; \succ \;\; a_1 \;\; \succ \;\; \dots\)

    \item The \textbf{well\hyp{}founded induction principle} then
      states that, for any predicate~\(\aleph\), \(\forall a \in
      A.\aleph(a)\) is implied by \(\forall a.(\forall b.a \; \succ \;
      b \;\; \Rightarrow \;\; \aleph(b)) \;\; \Rightarrow \;\;
      \aleph(a)\).

    \item Because there are no infinite descending chains, any subset
      \(B \;\; \subseteq \;\; A\) contains minimal elements \(M \;\;
      \subseteq \;\; B\), that is, there is no \(b \;\; \in \;\; B\)
      such that \(a \;\; \succ \;\; b\), if \(a \;\; \in \;\; M\).

    \item In this case, proving by well\hyp{}founded induction
      degenerates into proving \(\aleph(a)\) for all \(a \;\; \in \;\;
      M\).

    \item When \(A = \mathbb{N}\), this principle is called
      \textbf{mathematical (complete) induction}.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Well\hyp{}founded induction}

  \begin{itemize}

    \item \textbf{Structural induction} is another particular case
      where \(t \;\; \succ \;\; s\) holds if, and only if, \(s\)~is a
      proper \textbf{subterm} of~\(t\), namely, the abstract syntax
      tree of~\(s\) is included in the tree of~\(t\) and \(s \;\; \neq
      \;\; t\).

    \item Sometimes, a restricted form is enough. For instance, we can
      define \(\cons{x}{s} \;\; \succ \;\; s\), for any term~\(x\) and
      any stack~\(s \;\; \in \;\; S\). (Both \(x\)~and~\(s\) are
      \textbf{immediate subterms} of \(\cons{x}{s}\).)

    \item There is no infinite descending chain since \(\el\)~is the
      unique minimal element of~\(S\): no~\(s\) satisfies \(\el \;\;
      \succ \;\; s\); so the basis is \(t=\el\) and \(\forall
      t.(\forall s.t \; \succ \; s \;\; \Rightarrow \;\; \aleph(s))
      \;\; \Rightarrow \;\; \aleph(t)\) degenerates into
      \(\aleph(\el)\).

    \item In other words, when proving by induction on the length of a
      list, the \textbf{base case} is the empty list.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Proving associativity}

  \begin{itemize}

    \item Let us prove the \textbf{associativity} of the function
      ``\fun{cat}'', which we express formally by
      \begin{equation*}
        \pred{CatAssoc}{s,t,u} \colon \fun{cat}(s,\fun{cat}(t,u))
        \;\; \equiv \;\; \fun{cat}(\fun{cat}(s,t),u)
      \end{equation*}
      where \(s\), \(t\) and~\(u\) are stacks.

    \item We apply the well\hyp{}founded induction principle to the
      structure of~\(s\), so we must establish
      \begin{itemize}

        \item the basis \(\forall t,u \in S.\pred{CatAssoc}{\el,t,u}\)
          and

        \item step \(\forall s,t,u \in S.\pred{CatAssoc}{s,t,u} \;\;
          \Rightarrow \;\; \forall x \; \in \;
          T.\pred{CatAssoc}{\cons{x}{s},t,u}\).

      \end{itemize}

    \item The base case is direct:
      \begin{equation*}
        \fun{cat}(\el,\fun{cat}(t,u)) \;\; \xrightarrow{\alpha} \;\;
        \fun{cat}(t,u) \;\; \xleftarrow{\alpha} \;\;
        \fun{cat}(\fun{cat}(\el,t),u)
      \end{equation*}

    \item Let us assume now \(\pred{CatAssoc}{s,t,u}\), called the
      \textbf{induction hypothesis}.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Proving associativity}

  \begin{itemize}

    \item Let us prove \(\pred{CatAssoc}{\cons{x}{s},t,u}\), for any
      term~\(x\).

    \item The goal here is to use the rewrite system as an abstract
      machine to prove a property, with the timely help of the
      induction principle.

    \item Precisely, we want to rewrite each side of the equivalence
      we wish to prove until we either find the same term (equality),
      or we use the induction hypothesis (equivalence).

    \item Since we are going to work with equivalences, we should lift
      the call\hyp{}by\hyp{}value reduction strategy here, as it does
      not matter when arguments are evaluated, as long as their
      evaluation terminates.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Unification}

  \begin{itemize}

    \item We may start with the left\hyp{}hand side of
      \(\pred{CatAssoc}{\cons{x}{s},t,u}\), that is
      \(\fun{cat}(\cons{x}{s},\fun{cat}(t,u))\).

    \item We notice that this is \textbf{not} a value, as the call
      contains a reducible call, \(\fun{cat}(t,u)\).

    \item When using functions to compute, we match a call containing
      only values against the patterns of the rules.

    \item Here, we would like to match a pattern against a pattern.

    \item This is more general than matching, because values are
      patterns, and is called \textbf{unification}.

    \item Given two patterns, we want to find whether we can find
      assignments to the variables of these so that, when they are
      substituted by their denotation, the two patterns become equal.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Unification}

  \begin{itemize}

    \item Here, we would like to unify
      \(\fun{cat}(\cons{x}{s},\fun{cat}(t,u))\) (called the
      \textbf{goal}) and \(\fun{cat}(\cons{x}{s},t)\) (the pattern).

    \item The problem is that we have variables that are the same in
      the goal and the pattern (\(x\), \(s\)~and~\(t\)).

    \item The case of \(\cons{x}{s}\) and \(\cons{x}{s}\) is harmless,
      but the unification of \(\fun{cat}(t,u)\) and~\(t\) is not.

    \item That is why it is best to rename the variables in the
      pattern so they are not found in the goal (this is the
      \textbf{\(\alpha\)\hyp{}renaming} of \(\lambda\)\hyp{}calculus),
      and then try to unify them.

    \item To simplify things even more, we are going to rename
      \emph{all} the variables in the definition of ``\fun{cat}''.

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Unification}

  \begin{itemize}

    \item Let us try for example the following renaming
      \begin{align*}
        \fun{cat}(        \el,b) &\xrightarrow{\alpha} b\\
        \fun{cat}(\cons{i}{a},b) &\xrightarrow{\smash{\beta}}
        \cons{i}{\fun{cat}(a,b)}
      \end{align*}

    \item We want to unify the goal
      \(\fun{cat}(\cons{x}{s},\fun{cat}(t,u))\) with the pattern
      \(\fun{cat}(\cons{i}{a},b)\).

    \item The unification yields the equations \(x = i\), \(s = a\)
      and \(\fun{cat}(t,u) = b\).

    \item Note that we do not obtain assignments, as with pattern
      matching, but \textbf{equations}. An assignment binds a term to
      a variable: it is an asymmetric relation, whereas an equation
      symmetrically relates two terms.

    \item Then we apply the set of equations from the unification, so
      the right\hyp{}hand side of~\(\beta\),
      \(\cons{i}{\fun{cat}(a,b)}\), becomes
      \(\cons{x}{\fun{cat}(s,\fun{cat}(t,u))}\).

  \end{itemize}

\end{slide}

\begin{slide}
  \title{Unification}

  \begin{itemize}

    \item We have now proved the first step below:
      \begin{equation*}
        \begin{array}{r@{\;}l@{\;}l@{\qquad}r@{}}
          \fun{cat}(\cons{x}{s},\fun{cat}(t,u))
          & \xrightarrow{\beta}
          & \cons{x}{\fun{cat}(s,\fun{cat}(t,u))}\\
          & \equiv
          & \cons{x}{\fun{cat}(\fun{cat}(s,t),u)}
          & (\pred{CatAssoc}{s,t,u})\\
          & \xleftarrow{\smash\beta}
          & \fun{cat}(\cons{x}{\fun{cat}(s,t)},u)\\
          & \xleftarrow{\smash\beta}
          & \fun{cat}(\fun{cat}(\cons{x}{s},t),u).
        \end{array}
      \end{equation*}

    \item Note the use of the inductive hypothesis
      \(\pred{CatAssoc}{s,t,u}\).

    \item This derivation above establishes that
      \(\pred{CatAssoc}{\cons{x}{s},t,u}\) holds.

    \item The induction principle entails that \(\forall s,t,u \in
      S.\pred{CatAssoc}{s,t,u}\).\hfill\(\Box\)

    \item We have seen earlier that the cost of \(\fun{cat}(s,t)\) is
      \(\len{s} + 1\), where \(\len{s}\) is the length of~\(s\). So
      the cost of \(\fun{cat}(s,\fun{cat}(t,u))\) is \(\len{s} +
      \len{t} + 2\), whereas the cost of
      \(\fun{cat}(\fun{cat}(s,t),u)\) is \(2 \cdot \len{s} + \len{t} +
      2\).

    \item Thanks to our equivalence theorem, a compiler could replace
      the latter expression by the former to \textbf{optimise} the
      program.

  \end{itemize}

\end{slide}


\end{document}
